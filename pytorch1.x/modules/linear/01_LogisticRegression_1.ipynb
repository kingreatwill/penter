{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 逻辑回归\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to F:/bigdata/ai/Pytorch_Datasets/FashionMNIST\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31e565c613f6403a89249dd65ce1f526"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mnist_train = torchvision.datasets.FashionMNIST(root='F:/bigdata/ai/Pytorch_Datasets/FashionMNIST', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='F:/bigdata/ai/Pytorch_Datasets/FashionMNIST', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt（T恤）', 'trouser（裤子）', 'pullover（套衫）', 'dress（连衣裙）', 'coat（外套）',\n",
    "                   'sandal（凉鞋）', 'shirt（衬衫）', 'sneaker（运动鞋）', 'bag（包）', 'ankle boot（短靴）']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 传统的逻辑回归(二分类)\n",
    "定义模型\n",
    "```\n",
    "class LR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LR,self).__init__()\n",
    "        self.fc=nn.Linear(24,2) # 由于24个维度已经固定了，所以这里写24\n",
    "    def forward(self,x):\n",
    "        out=self.fc(x)\n",
    "        out=torch.sigmoid(out)\n",
    "        return out\n",
    "```\n",
    "测试集上的准确率\n",
    "```\n",
    "def test(pred,lab):\n",
    "    t=pred.max(-1)[1]==lab\n",
    "    return torch.mean(t.float())\n",
    "```\n",
    "\n",
    "```\n",
    "net=LR()\n",
    "criterion=nn.CrossEntropyLoss() # 使用CrossEntropyLoss损失\n",
    "optm=torch.optim.Adam(net.parameters()) # Adam优化\n",
    "epochs=1000 # 训练1000次\n",
    "# test_lab 属于{0,1}\n",
    "\n",
    "for i in range(epochs):\n",
    "    # 指定模型为训练模式，计算梯度\n",
    "    net.train()\n",
    "    # 输入值都需要转化成torch的Tensor(这里是做一次全量的，没有分batch)\n",
    "    x=torch.from_numpy(train_data).float()\n",
    "    y=torch.from_numpy(train_lab).long()\n",
    "    y_hat=net(x)\n",
    "    loss=criterion(y_hat,y) # 计算损失\n",
    "    optm.zero_grad() # 前一步的损失清零\n",
    "    loss.backward() # 反向传播\n",
    "    optm.step() # 优化\n",
    "    if (i+1)%100 ==0 : # 这里我们每100次输出相关的信息\n",
    "        # 指定模型为计算模式\n",
    "        net.eval()\n",
    "        test_in=torch.from_numpy(test_data).float()\n",
    "        test_l=torch.from_numpy(test_lab).long()\n",
    "        test_out=net(test_in)\n",
    "        # 使用我们的测试函数计算准确率\n",
    "        accu=test(test_out,test_l)\n",
    "        print(\"Epoch:{},Loss:{:.4f},Accuracy：{:.2f}\".format(i+1,loss.item(),accu))\n",
    "```\n",
    "\n",
    "# 如果说自己实现（多分类）\n",
    "```\n",
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(dim=1, keepdim=True)\n",
    "    return X_exp / partition  # 这里应用了广播机制\n",
    "\n",
    "def net(X):\n",
    "    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)\n",
    "\n",
    "# 交叉熵损失函数\n",
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat.gather(1, y.view(-1, 1)))\n",
    "\n",
    "# 分类准确率\n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "# 评价模型net在数据集data_iter上的准确率\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "```\n",
    "训练模型\n",
    "\n",
    "```\n",
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "\n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step()  # “softmax回归的简洁实现”一节将用到\n",
    "\n",
    "\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}