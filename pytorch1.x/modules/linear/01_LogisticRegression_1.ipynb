{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 逻辑回归\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE' # plt绘图报错（不显示图的问题）\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei'] #用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus']=False #用来正常显示负号"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.FashionMNIST(root='F:/bigdata/ai/Pytorch_Datasets',\n",
    "                                                train=True,\n",
    "                                                download=True,\n",
    "                                                transform=transforms.ToTensor())\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='F:/bigdata/ai/Pytorch_Datasets',\n",
    "                                               train=False,\n",
    "                                               download=True,\n",
    "                                               transform=transforms.ToTensor())\n",
    "# 会在指定目录下自动创建FashionMNIST文件夹（目录后面不要加/）\n",
    "\n",
    "def get_fashion_mnist_labels(labels):\n",
    "    text_labels = ['t-shirt（T恤）', 'trouser（裤子）', 'pullover（套衫）', 'dress（连衣裙）', 'coat（外套）',\n",
    "                   'sandal（凉鞋）', 'shirt（衬衫）', 'sneaker（运动鞋）', 'bag（包）', 'ankle boot（短靴）']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "\n",
    "# len(images) >= 2\n",
    "def show_fashion_mnist(images, labels):\n",
    "    # 这里的_表示我们忽略（不使用）的变量\n",
    "    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n",
    "    for f, img, lbl in zip(figs, images, labels):\n",
    "        f.imshow(img.view((28, 28)).numpy())\n",
    "        f.set_title(lbl)\n",
    "        f.axes.get_xaxis().set_visible(False)\n",
    "        f.axes.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "# 计算准确率\n",
    "def evaluate_accuracy(test_data_iter, model):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in test_data_iter:\n",
    "        acc_sum += (model(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 864x864 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mnist_test[0][0].size()) #(1,28,28)\n",
    "\n",
    "show_fashion_mnist([mnist_test[0][0],mnist_test[1][0]], get_fashion_mnist_labels([mnist_test[0][1],mnist_test[1][1]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_inputs, num_outputs)\n",
    "        )\n",
    "        # self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "    def forward(self, x): # x shape: (batch, 1, 28, 28)\n",
    "        # y = self.linear(x.view(x.shape[0], -1))\n",
    "        y = self.linear(x)\n",
    "        # y = torch.softmax(y,dim=1) #不需要，加了效果反而不好\n",
    "        return y\n",
    "\n",
    "model = LogisticRegression(num_inputs, num_outputs)\n",
    "init.normal_(model.linear[1].weight, mean=0, std=0.01)\n",
    "init.constant_(model.linear[1].bias, val=0)\n",
    "\n",
    "# nn.CrossEntropyLoss()中的label不需要是one_hot。要求是一维的label。\n",
    "# 在使用该函数前 input不需要经过softmax计算， target不是one_hot编码格式\n",
    "loss = nn.CrossEntropyLoss() \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.560904, acc: 0.786200\n",
      "epoch 2, loss: 0.491178, acc: 0.811400\n",
      "epoch 3, loss: 0.463886, acc: 0.819500\n",
      "epoch 4, loss: 0.447283, acc: 0.825000\n",
      "epoch 5, loss: 0.435161, acc: 0.826800\n",
      "epoch 6, loss: 0.425506, acc: 0.829100\n",
      "epoch 7, loss: 0.417439, acc: 0.832200\n",
      "epoch 8, loss: 0.410499, acc: 0.833700\n",
      "epoch 9, loss: 0.404409, acc: 0.835100\n",
      "epoch 10, loss: 0.398988, acc: 0.835800\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(mnist_train, batch_size=batch_size,shuffle=True)\n",
    "test_dataloader = DataLoader(mnist_test, batch_size=batch_size)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    for X, y in train_dataloader:\n",
    "        # 计算预测误差\n",
    "        output = model(X)\n",
    "        # y = torch.nn.functional.one_hot(y,10) # 不需要\n",
    "        l = loss(output, y)\n",
    "\n",
    "        # 反向传播\n",
    "        optimizer.zero_grad() # 梯度清零，等价于net.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    print('epoch %d, loss: %f, acc: %f' % (epoch, l.item() , evaluate_accuracy(test_dataloader,model)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9]) ---实际： 9\n",
      "tensor([2]) ---实际： 2\n",
      "tensor([1]) ---实际： 1\n",
      "tensor([1]) ---实际： 1\n",
      "tensor([6]) ---实际： 6\n",
      "tensor([1]) ---实际： 1\n",
      "tensor([4]) ---实际： 4\n",
      "tensor([6]) ---实际： 6\n",
      "tensor([5]) ---实际： 5\n",
      "tensor([7]) ---实际： 7\n",
      "tensor([4]) ---实际： 4\n",
      "tensor([5]) ---实际： 5\n",
      "tensor([5]) ---实际： 7\n",
      "tensor([3]) ---实际： 3\n",
      "tensor([4]) ---实际： 4\n",
      "tensor([1]) ---实际： 1\n",
      "tensor([2]) ---实际： 2\n",
      "tensor([4]) ---实际： 4\n",
      "tensor([8]) ---实际： 8\n",
      "tensor([0]) ---实际： 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    #print(np.argmax(model(mnist_test[i][0]).detach().numpy()) ,\"---实际：\", mnist_test[i][1] )\n",
    "    print(model(mnist_test[i][0]).argmax(dim=1) ,\"---实际：\", mnist_test[i][1] )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 传统的逻辑回归(二分类)\n",
    "定义模型\n",
    "```\n",
    "class LR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LR,self).__init__()\n",
    "        self.fc=nn.Linear(24,2) # 由于24个维度已经固定了，所以这里写24\n",
    "    def forward(self,x):\n",
    "        out=self.fc(x)\n",
    "        out=torch.sigmoid(out)\n",
    "        return out\n",
    "```\n",
    "测试集上的准确率\n",
    "```\n",
    "def test(pred,lab):\n",
    "    t=pred.max(-1)[1]==lab\n",
    "    return torch.mean(t.float())\n",
    "```\n",
    "\n",
    "```\n",
    "net=LR()\n",
    "criterion=nn.CrossEntropyLoss() # 使用CrossEntropyLoss损失\n",
    "optm=torch.optim.Adam(net.parameters()) # Adam优化\n",
    "epochs=1000 # 训练1000次\n",
    "# test_lab 属于{0,1}\n",
    "\n",
    "for i in range(epochs):\n",
    "    # 指定模型为训练模式，计算梯度\n",
    "    net.train()\n",
    "    # 输入值都需要转化成torch的Tensor(这里是做一次全量的，没有分batch)\n",
    "    x=torch.from_numpy(train_data).float()\n",
    "    y=torch.from_numpy(train_lab).long()\n",
    "    y_hat=net(x)\n",
    "    loss=criterion(y_hat,y) # 计算损失\n",
    "    optm.zero_grad() # 前一步的损失清零\n",
    "    loss.backward() # 反向传播\n",
    "    optm.step() # 优化\n",
    "    if (i+1)%100 ==0 : # 这里我们每100次输出相关的信息\n",
    "        # 指定模型为计算模式\n",
    "        net.eval()\n",
    "        test_in=torch.from_numpy(test_data).float()\n",
    "        test_l=torch.from_numpy(test_lab).long()\n",
    "        test_out=net(test_in)\n",
    "        # 使用我们的测试函数计算准确率\n",
    "        accu=test(test_out,test_l)\n",
    "        print(\"Epoch:{},Loss:{:.4f},Accuracy：{:.2f}\".format(i+1,loss.item(),accu))\n",
    "```\n",
    "\n",
    "# 如果说自己实现（多分类）\n",
    "```\n",
    "def softmax(X):\n",
    "    X_exp = X.exp()\n",
    "    partition = X_exp.sum(dim=1, keepdim=True)\n",
    "    return X_exp / partition  # 这里应用了广播机制\n",
    "\n",
    "def net(X):\n",
    "    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)\n",
    "\n",
    "# 交叉熵损失函数\n",
    "def cross_entropy(y_hat, y):\n",
    "    return - torch.log(y_hat.gather(1, y.view(-1, 1)))\n",
    "\n",
    "# 分类准确率\n",
    "def accuracy(y_hat, y):\n",
    "    return (y_hat.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "# 评价模型net在数据集data_iter上的准确率\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for X, y in data_iter:\n",
    "        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n\n",
    "```\n",
    "训练模型\n",
    "\n",
    "```\n",
    "num_epochs, lr = 5, 0.1\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, optimizer=None):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y).sum()\n",
    "\n",
    "            # 梯度清零\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "            elif params is not None and params[0].grad is not None:\n",
    "                for param in params:\n",
    "                    param.grad.data.zero_()\n",
    "\n",
    "            l.backward()\n",
    "            if optimizer is None:\n",
    "                d2l.sgd(params, lr, batch_size)\n",
    "            else:\n",
    "                optimizer.step()  # “softmax回归的简洁实现”一节将用到\n",
    "\n",
    "\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))\n",
    "\n",
    "train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}