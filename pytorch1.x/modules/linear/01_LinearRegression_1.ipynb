{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits import mplot3d # 需要的\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 线性回归\n",
    "\n",
    "## 生成数据集\n",
    "用权重w=[2, -3.4] 和偏差b=4.2,特征2维， 来生成标签y~N(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "# size = num_examples * num_inputs\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = torch.tensor([4.2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [],
   "source": [
    "features = torch.randn(num_examples, num_inputs,\n",
    "                       dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "# y = xw + b\n",
    "# labels_without_noise_1 = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels_without_noise = features@true_w.t() + true_b"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-206-d7fcd44177cf>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels_noise = torch.tensor(torch.normal(0, 0.01, size=labels_without_noise.size()),\n"
     ]
    }
   ],
   "source": [
    "# y = xw+b + 高斯噪音\n",
    "# np.random.normal(0, 0.01, size=labels_without_noise.size())\n",
    "labels_noise = torch.tensor(torch.normal(0, 0.01, size=labels_without_noise.size()),\n",
    "                       dtype=torch.float32)\n",
    "\n",
    "labels_with_noise = labels_without_noise+labels_noise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "outputs": [],
   "source": [
    "# ax = plt.axes(projection='3d')\n",
    "# # 三维散点的数据\n",
    "# ax.scatter3D(features[:,0], features[:,1], labels_with_noise, c=labels_with_noise, cmap='Greens')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3704,  1.9186],\n",
      "        [-0.3308, -1.0099],\n",
      "        [-0.1591,  0.1344],\n",
      "        [-2.7629,  0.7443],\n",
      "        [-0.3827,  1.3340],\n",
      "        [-0.6479, -1.5602],\n",
      "        [-0.1737,  2.3560],\n",
      "        [-0.5693,  0.0557],\n",
      "        [-0.8811,  0.6568],\n",
      "        [-0.5456, -1.4293]]) tensor([-1.5927,  6.9677,  3.4109, -3.8602, -1.1135,  8.2250, -4.1563,  2.8568,\n",
      "         0.2125,  7.9604])\n"
     ]
    }
   ],
   "source": [
    "# 它每次返回batch_size（批量大小）个随机样本的特征和标签。\n",
    "def data_iter(batch_size, features, labels):\n",
    "    num_examples = len(features)\n",
    "    indices = list(range(num_examples))\n",
    "    random.shuffle(indices)  # 样本的读取顺序是随机的\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # 最后一次可能不足一个batch\n",
    "        yield  features.index_select(0, j), labels.index_select(0, j)\n",
    "\n",
    "\n",
    "for X, y in data_iter(10, features, labels_with_noise):\n",
    "    print(X, y)\n",
    "    break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "def linreg(X, w, b):\n",
    "    return torch.mm(X, w) + b #X@w + b\n",
    "\n",
    "# 定义损失函数\n",
    "def squared_loss(y_hat, y):\n",
    "    # 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2\n",
    "    return (y_hat - y.view(y_hat.size())) ** 2 / 2\n",
    "\n",
    "# 定义优化算法\n",
    "def sgd(params, lr, batch_size):\n",
    "    for param in params:\n",
    "        param.data -= lr * param.grad / batch_size # 注意这里更改param时用的param.data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.], requires_grad=True)"
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)\n",
    "# b = torch.zeros(1, dtype=torch.float32)\n",
    "# 初始化参数：可以随机\n",
    "w = torch.tensor([[0],[0]],dtype=torch.float32,requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float32)\n",
    "b.requires_grad_(requires_grad=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.048617\n",
      "epoch 2, loss 0.000199\n",
      "epoch 3, loss 0.000053\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "lr = 0.03\n",
    "batch_size = 10\n",
    "num_epochs = 3 # 完整数据集迭代次数\n",
    "net = linreg\n",
    "loss = squared_loss\n",
    "\n",
    "for epoch in range(num_epochs):  # 训练模型一共需要num_epochs个迭代周期\n",
    "    # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X\n",
    "    # 和y分别是小批量样本的特征和标签\n",
    "    for X, y in data_iter(batch_size, features, labels_with_noise):\n",
    "        l = loss(net(X, w, b), y).sum()  # l是有关小批量X和y的损失\n",
    "        l.backward()  # 小批量的损失对模型参数求梯度\n",
    "        sgd([w, b], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数\n",
    "\n",
    "        # 不要忘了梯度清零\n",
    "        w.grad.data.zero_()\n",
    "        b.grad.data.zero_()\n",
    "    train_l = loss(net(features, w, b), labels_with_noise)\n",
    "    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0000, -3.4000]) ~ tensor([[ 1.9997],\n",
      "        [-3.3989]], requires_grad=True)\n",
      "tensor([4.2000]) ~ tensor([4.1995], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(true_w, '~', w)\n",
    "print(true_b, '~', b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}