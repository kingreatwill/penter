{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 神经网络模型\n",
    "\n",
    "## MLPRegressor & MLPClassifier 多层感知器 - 监督学习\n",
    "> 此实现不适用于大规模应用\n",
    "> 建议将输入向量X上的每个属性缩放为[0，1]或[-1，+1]，或将其标准化为均值0和方差1 （StandardScaler）\n",
    "> 多层感知机的层与层之间是全连接的（全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接）\n",
    "\n",
    "自己迭代次数\n",
    "```\n",
    "clf = MLPClassifier(max_iter=1, warm_start=True)\n",
    "for i in range(10):\n",
    "    clf.fit(X, y)\n",
    "```\n",
    "- 参数： warm_start，就是保留一定的上一次运行时的参数，避免重新设置参数把模型跑乱了。\n",
    "1、如果warm_start=True就表示就是在模型训练的过程中，在前一阶段的训练结果上继续训练\n",
    "2、如果warm_start=False就表示从头开始训练模型\n",
    "- 参数：max_iter: int，可选，默认200，最大迭代次数。直到收敛（由“ tol”确定）或达到此迭代次数为止。\n",
    "- 参数：hidden_layer_sizes :例如hidden_layer_sizes=(50, 50)，表示有两层隐藏层，第一层隐藏层有50个神经元，第二层也有50个神经元。\n",
    "- 参数：activation :激活函数，激活隐藏层,{‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, 默认relu（激活函数：有点像用针扎你，使多大的劲 你才会疼？才能激活你的反应？）\n",
    "- 参数：solver： {‘lbfgs’, ‘sgd’, ‘adam’}, 默认adam，用来优化权重 ；‘adam’在相对较大的数据集上效果比较好（几千个样本或者更多），对小数据集来说，lbfgs收敛更快效果也更好。\n",
    "- 参数：alpha: L2惩罚（正则项）参数。\n",
    "- 参数：batch_size : int , 可选的，默认’auto’,随机优化的minibatches的大小batch_size=min(200,n_samples)，如果solver是’lbfgs’，分类器将不使用minibatch \n",
    "- 参数：learning_rate :学习率,用于权重更新,只有当solver为’sgd’时使用，{‘constant’，’invscaling’, ‘adaptive’},默认constant（Only used when solver='sgd'.）\n",
    "  - ‘constant’: 有’learning_rate_init’给定的恒定学习率 \n",
    "  - ‘incscaling’：随着时间t使用’power_t’的逆标度指数不断降低学习率learning_rate_ ，effective_learning_rate = learning_rate_init / pow(t, power_t) \n",
    "  - ‘adaptive’：只要训练损耗在下降，就保持学习率为’learning_rate_init’不变，当连续两次不能降低训练损耗或验证分数停止升高至少tol时，将当前学习率除以5. \n",
    "- 参数：learning_rate_int:double,可选，默认0.001，初始学习率，控制更新权重的补偿，只有当solver=’sgd’ 或’adam’时使用。\n",
    "- 参数：power_t: double, 可选, default 0.5，只有solver=’sgd’时使用，是逆扩展学习率的指数.当learning_rate=’invscaling’，用来更新有效学习率。 \n",
    "- 参数：shuffle: bool，可选，默认True,只有当solver=’sgd’或者‘adam’时使用，判断是否在每次迭代时对样本进行清洗。\n",
    "- 参数：tol：float, 可选，默认1e-4，优化的容忍度，除非将learning_rate设置为“ adaptive”，否则将认为达到收敛并停止训练。\n",
    "- 参数：verbose : bool, 可选, 默认False,是否将过程打印到stdout\n",
    "- 参数：momentum：默认 0.9,动量梯度下降更新，设置的范围应该0.0-1.0. 只有solver=’sgd’时使用.\n",
    "- 参数：nesterovs_momentum : boolean, 默认True, Whether to use Nesterov’s momentum. 只有solver=’sgd’并且momentum > 0使用.\n",
    "- 参数：early_stopping : bool, 默认False,只有solver=’sgd’或者’adam’时有效,判断当验证效果不再改善的时候是否终止训练，当为True时，自动选出10%的训练数据用于验证并在两步连续迭代改善，低于tol时终止训练。 \n",
    "防止过拟合，[EARLY STOPPING是什么](https://www.freesion.com/article/73701316398/)\n",
    "- 参数：validation_fraction : float, 可选, 默认 0.1,用作早期停止验证的预留训练数据集的比例，早0-1之间，只当early_stopping=True有用\n",
    "- 参数：beta_1 : float, 可选, 默认0.9，只有solver=’adam’时使用，估计一阶矩向量的指数衰减速率，[0,1)之间\n",
    "- 参数： beta_2 : float, 可选, 默认0.999,只有solver=’adam’时使用估计二阶矩向量的指数衰减速率[0,1)之间\n",
    "- 参数：epsilon : float, 可选, 默认1e-8,只有solver=’adam’时使用数值稳定值。\n",
    "- 参数：n_iter_no_change, default=10，表示验证集的评分在多轮迭代中没有提高，则停止训练。当solver=’sgd’ or ‘adam’可用\n",
    "- 参数：max_fun, default=15000，当solver=’lbfgs’；损失函数调用的最大次数。\n",
    "\n",
    "属性说明： \n",
    "- classes_:每个输出的类标签 \n",
    "- loss_:损失函数计算出来的当前损失值 \n",
    "- coefs_:列表中的第i个元素表示i层的权重矩阵 \n",
    "- intercepts_:列表中第i个元素代表i+1层的偏差向量 \n",
    "- n_iter_ ：迭代次数 \n",
    "- n_layers_:层数 \n",
    "- n_outputs_:输出的个数 \n",
    "- out_activation_:输出激活函数的名称。 \n",
    "\n",
    "## BernoulliRBM\n",
    "基于概率模型的无监督非线性特征学习器\n",
    "```\n",
    "#创建RBM模型\n",
    "logistic = linear_model.LogisticRegression()\n",
    "rbm = BernoulliRBM(random_state=0, verbose=True) # 没有predict方法的\n",
    "classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}